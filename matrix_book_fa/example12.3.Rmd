---
title: "مثال ۱۲-۳: تخمین حداکثر درست‌نمایی (یک مثال کابردی برای بهینه‌یابی)"
author: "رامین مجاب"
output: md_document
---
 
 **(صفحهٔ ۳۳۹ کتاب)**
 
در این بخش، حل تحلیلی تخمین‌زنندهٔ حداکثر درست‌نمایی در مدل نرمال چندمتغیره به‌دست آمد؛ این یعنی محاسبهٔ این تخمین‌زننده برای یک مجموعه از مشاهدات ساده است. بااین‌حال، در این مثال از این اطلاعات استفاده نمی‌کنیم؛ در عوض، با توجه به تابع لگاریتم درست‌نمایی و فرایند بهینه‌یابی که در [مثال ۱۰-۱](matrix_book_fa_example10.1) بحث شد، محاسبات را انجام می‌دهیم. در واقع، همهٔ مسائل حداکثر درست‌نمایی حل تحلیلی ندارند. در بسیاری از موارد، لازم است که چنین فرایندی را برای آن‌ها پیاده‌سازی کنیم. 

نمونه‌ای که برای این مثال استفاده می‌کنیم  در [مثال ۱۱-۱](matrix_book_fa_example11.1) معرفی کردیم. هدف آن است که یک مدل نرمال چندمتغیره برای ویژگی‌های «مسافت به مایل با یک گالن»، «تعداد سیلندر»، و «وزن» داشته باشیم. در [مثال ۱۱-۲](matrix_book_fa_example11.2)، بردار میانگین و ماتریس واریانس را گزارش کرده‌ایم. پس،  حل تحلیلی تخمین‌زنندهٔ حداکثر درست‌نمایی  را داریم. در کد زیر، محاسبات را تکرار می‌کنیم:
```{r}
data <- datasets::mtcars[, c("mpg", "cyl", "wt")]
colMeans(data)
cov(data) * (n-1)/n
```

برای پیاده‌سازی فرایند بهینه‌یابی، در ابتدا لازم است محدودیت‌های مدل را لحاظ کنیم. یکی از این محدودیت‌ها متقارن‌بودن و مثبت معین‌بودن ماتریس واریانس است. برای آنکه به فرایند بهینه‌یابی بفهمانیم که ماتریس واریانس این‌گونه ساختاری دارد، یک راهکار عملیاتی آن است که تنها ارزش‌های قطر  و بالای قطر ماتریس را به این فرایند وارد کنیم و سپس، خودمان ماتریس اصلی را بسازیم. بنابراین، به کد زیر نیاز داریم:

```{r}
mat.from.vec <- function(vec) {
  n <- round((sqrt(8 * length(vec) + 1) - 1) / 2)
  mat <- matrix(0, nrow = n, ncol = n)
  for (k in 1:length(vec)) {
    i <- floor((sqrt(8 * (k - 1) + 1) - 1) / 2) + 1
    j <- k - i * (i - 1) / 2
    mat[i, j] <- vec[k]
    if (i != j) {
      mat[j, i] <- vec[k]
    }
  }
  mat
}
```

تابع فوق یک بردار را که می‌توانید آن را حالت فشردهٔ یک ماتریس متقارن ببینید، به ماتریس متقارن تبدیل می‌کند. منطق آن ساده است و به‌منظور خلاصه‌کردن بحث، در اینجا مطرح نمی‌شود. ممکن است راهکار دیگر، تخمین تابع بر اساس عبارت $\mathbf{S}'\mathbf{S}$ باشد. در این حالت، مطمئن هستیم که علاوه بر متقارن‌بودن، ماتریس واریانس مثبت معین نیز است. البته، در این حالت تعداد پارامترهای بیشتری را تخمین می‌زنیم (به‌عنوان تمرین، انجام دهید).

همچنین، به تابعی نیاز داریم که لگاریتم درست‌نمایی را از پارامترها، مشاهدات، و دیگر اطلاعات محاسبه کند. با در اختیار داشتن فرمول ۱۲-۳۴ (صفحهٔ ۳۳۸ کتاب) و درنظرگرفتن ساختار مشخصی برای بردار پارامترها، این کار ساده است. کد زیر را ملاحظه کنید:

```{r}

loglik <- function(par, data, m) {
  mu <- par[1:m]
  S <- mat.from.vec(par[(m + 1):(m + m * (m + 1) / 2)])
  d_S <- det(S)
  if (d_S < 1e-10) {
    return(1e10)
  }
  S_i <- solve(S)
  n <- nrow(data)
  -n / 2 * log(d_S) - 1 / 2 *
    sum(apply(data, 1, function(x) {
      t(x - mu) %*% S_i %*% (x - mu)
    }))
}

```

ساختار بردار پارامترها را این‌گونه در نظر گرفته‌ایم که درایه‌های اول برابر با بردار میانگین و پس از آن فرم فشرده‌شدهٔ ماتریس واریانس قرار گیرد. از نکات مهم در  نوشتن تابع هدف آن است که بدانیم الگوریتم حداقل‌سازی یا حداکثرسازی است. در اینجا، علامت منفی در انتهای تابع برای سازگارکردن تابع هدف با الگوریتم نوشته شده است. پس، باید دقت کرد که آنچه این تابع بازمی‌گرداند، قرینهٔ لگاریتم درست‌نمایی است. از نکات دیگر آن است که باید وضعیتی که الگوریتم با خطا مواجه می‌شود، پیش‌بینی و اگر این وضعیت، به‌دلیل انتخاب بردار پارامتر نامناسب رخ داده است، آن را مدیریت کنیم؛ مثلاً در کد فوق، معکوس‌پذیری ماتریس واریانس را مدیریت کرده‌ایم.

اکنون، برای بهینه‌یابی تنها به یک مقدار اولیه نیازمندیم. کد زیر را ببینید:

```{r}
m <- ncol(data)
par <- c(colMeans(data), cov(data)[upper.tri(cov(data), 
                                             diag = TRUE
)]) + 0.2
result <- optim(par, loglik, data = data, m = m, method = "BFGS")

print(result$par[1:3]) # mean

print(mat.from.vec(
  result$par[(m + 1):(m + m * (m + 1) / 2)]
)) # covariance

```

نقاط اولیه در کد فوق را نزدیک به نقطهٔ بهینه انتخاب کرده‌ایم. به‌عنوان تمرین، حساسیت نتیجهٔ بهینه‌یابی و فاصله‌ای که آن از جواب حل تحلیلی می‌گیرد،  با دورترکردن این نقطهٔ اولیه از جواب بررسی کنید. اگر این تمرین را انجام دهید، متوجه نقش اساسی نقطهٔ اولیه خواهید شد. در مسائل بزرگ‌تر و پیچیده‌تر، الزاماً باید نقطهٔ اولیهٔ مناسبی نزدیک به نقطهٔ بهینه بیابیم؛ به‌عنوان مثال، وقتی تخمین‌زنندهٔ حداکثر درست‌نمایی کاراتر از تخمین‌زنندهٔ حداقل مربعات است، از دومی برای آغاز فرایند بهینه‌یابی اولی استفاده می‌شود.

به مدل‌سازی بازگردیم. به این پرسش توجه کنید که آیا اصولاً مدل مناسبی انتخاب کرده‌ایم؟ پاسخ احتمالاً منفی است، زیرا اینکه بخواهیم مصرف سوخت در هر مایل یک ماشین را با   فرض مدل نرمال چندمتغیره مدل‌سازی کنیم، انتخاب مناسبی نیست. مصرف سوخت هیچ‌گاه منفی نیست و این مدل، این محدودیت را لحاظ نمی‌کند؛ پس باید در جست‌وجوی مدل مناسب‌تری باشیم. نظرتان دربارهٔ مدل‌سازی لگاریتم مشاهدات با استفاده از توزیع نرمال چندمتغیره چیست؟

	

